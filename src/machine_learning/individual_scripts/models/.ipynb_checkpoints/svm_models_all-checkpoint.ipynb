{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8396c1f-fede-488d-af7b-e577d5b6f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, r2_score, \\\n",
    "roc_curve, auc, precision_score, recall_score, confusion_matrix, precision_recall_curve\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50ce19dd-e215-41c4-8dd4-0787b0e02761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS #\n",
    "seed = 42\n",
    "splits = 10 # from what we decided\n",
    "# protein_type = 'linear' # make equal to 'linear' or 'log' and variables will update accordingly\n",
    "# subset = 'full' # full or type of feature selected\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "data_dir = os.path.join(parent_dir, 'datasets')\n",
    "\n",
    "# Initialize lists to store data splits\n",
    "X_train_list, X_test_list = [], []\n",
    "y_train_list, y_test_list = [], [] # same response variables no matter the X transformation\n",
    "\n",
    "# Iterate over each split index\n",
    "for i in range(1, splits + 1):\n",
    "    train_file = os.path.join(data_dir, f'train_{i}.csv')\n",
    "    test_file = os.path.join(data_dir, f'test_{i}.csv')\n",
    "\n",
    "    # Read training data and split into X_train and y_train\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    X_train = train_df.drop(columns=['mtx_binary', 'Unnamed: 0', 'EAC_ID'])\n",
    "    y_train = train_df['mtx_binary']\n",
    "    \n",
    "    # Read test data and split into X_test and y_test\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    X_test = test_df.drop(columns=['mtx_binary', 'Unnamed: 0', 'EAC_ID'])\n",
    "    y_test = test_df['mtx_binary']\n",
    "    \n",
    "    # Append to respective lists\n",
    "    X_train_list.append(X_train)\n",
    "    X_test_list.append(X_test)\n",
    "    y_train_list.append(y_train)\n",
    "    y_test_list.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7617193e-fef9-4617-ad86-6bc3874939a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 - AUC: 0.22222222222222224, Accuracy: 0.16666666666666666, Precision: 0.25, TPR: 0.3333333333333333, FPR: 1.0\n",
      "Split 1 - AUC: 0.7777777777777779, Accuracy: 0.8333333333333334, Precision: 0.75, TPR: 1.0, FPR: 0.3333333333333333\n",
      "Split 2 - AUC: 0.888888888888889, Accuracy: 0.6666666666666666, Precision: 0.6, TPR: 1.0, FPR: 0.6666666666666666\n",
      "Split 3 - AUC: 0.75, Accuracy: 0.6666666666666666, Precision: 0.6666666666666666, TPR: 1.0, FPR: 1.0\n",
      "Split 4 - AUC: 0.5, Accuracy: 0.5, Precision: 0.6, TPR: 0.75, FPR: 1.0\n",
      "Split 5 - AUC: 0.875, Accuracy: 0.6666666666666666, Precision: 0.75, TPR: 0.75, FPR: 0.5\n",
      "Split 6 - AUC: 0.875, Accuracy: 0.8333333333333334, Precision: 1.0, TPR: 0.75, FPR: 0.0\n",
      "Split 7 - AUC: 0.875, Accuracy: 0.8333333333333334, Precision: 1.0, TPR: 0.75, FPR: 0.0\n",
      "Split 8 - AUC: 1.0, Accuracy: 1.0, Precision: 1.0, TPR: 1.0, FPR: 0.0\n",
      "Split 9 - AUC: 0.625, Accuracy: 0.6666666666666666, Precision: 0.6666666666666666, TPR: 1.0, FPR: 1.0\n",
      "Mean AUC:  0.7388888888888889\n",
      "Mean Accuracy:  0.6833333333333333\n",
      "Mean Precision:  0.7283333333333334\n",
      "Mean Recall (TPR):  0.8333333333333333\n",
      "Mean FPR:  0.55\n"
     ]
    }
   ],
   "source": [
    "# This is a linear svc model without any feature selection using all 10 folds\n",
    "auc_list = []\n",
    "acc_list = []\n",
    "prec_list = []\n",
    "tpr_list = []\n",
    "fpr_list = []\n",
    "# Loop through each split, train the model, and print metrics\n",
    "for i in range(len(X_train_list)):\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i]\n",
    "    \n",
    "    # implement model\n",
    "    model = LinearSVC(random_state=seed, dual=\"auto\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get decision function scores (predict_proba)\n",
    "    y_scores = model.decision_function(X_test) # linear svc does not have a direct predict proba attribute\n",
    "    # Predict labels\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # calculate metrics\n",
    "    # ROC AUC\n",
    "    fprs, tprs, thresholds = roc_curve(y_test, y_scores)\n",
    "    roc_auc = auc(fprs, tprs)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    tpr = recall_score(y_test, y_pred) # redefine as a value not a vector from before\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel() # retrieve to calculate fpr\n",
    "    fpr = fp / (fp + tn)\n",
    "    # append metrics for averages\n",
    "    acc_list.append(accuracy)\n",
    "    auc_list.append(roc_auc)\n",
    "    prec_list.append(precision)\n",
    "    tpr_list.append(tpr)\n",
    "    fpr_list.append(fpr)\n",
    "    \n",
    "    print(f\"Split {i} - AUC: {roc_auc}, Accuracy: {accuracy}, Precision: {precision}, TPR: {tpr}, FPR: {fpr}\")\n",
    "\n",
    "# Report avg metrics\n",
    "print(\"Mean AUC: \", np.mean(auc_list))\n",
    "print(\"Mean Accuracy: \", np.mean(acc_list))\n",
    "print(\"Mean Precision: \", np.mean(prec_list))\n",
    "print(\"Mean Recall (TPR): \", np.mean(tpr_list))\n",
    "print(\"Mean FPR: \", np.mean(fpr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53bea3e2-c2fe-4931-985e-8c63f8970c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 - AUC: 0.5555555555555556, Accuracy: 0.5, Precision: 0.5, TPR: 1.0, FPR: 1.0\n",
      "Split 1 - AUC: 0.7777777777777778, Accuracy: 0.5, Precision: 0.5, TPR: 1.0, FPR: 1.0\n",
      "Split 2 - AUC: 0.3333333333333333, Accuracy: 0.5, Precision: 0.5, TPR: 1.0, FPR: 1.0\n",
      "Split 3 - AUC: 0.25, Accuracy: 0.6666666666666666, Precision: 0.6666666666666666, TPR: 1.0, FPR: 1.0\n",
      "Split 4 - AUC: 0.625, Accuracy: 0.6666666666666666, Precision: 0.6666666666666666, TPR: 1.0, FPR: 1.0\n",
      "Split 5 - AUC: 0.875, Accuracy: 0.6666666666666666, Precision: 0.6666666666666666, TPR: 1.0, FPR: 1.0\n",
      "Split 6 - AUC: 1.0, Accuracy: 0.6666666666666666, Precision: 0.6666666666666666, TPR: 1.0, FPR: 1.0\n",
      "Split 7 - AUC: 0.875, Accuracy: 0.6666666666666666, Precision: 0.6666666666666666, TPR: 1.0, FPR: 1.0\n",
      "Split 8 - AUC: 0.75, Accuracy: 0.6666666666666666, Precision: 0.6666666666666666, TPR: 1.0, FPR: 1.0\n",
      "Split 9 - AUC: 0.0, Accuracy: 0.6666666666666666, Precision: 0.6666666666666666, TPR: 1.0, FPR: 1.0\n",
      "Mean AUC:  0.6041666666666667\n",
      "Mean Accuracy:  0.6166666666666667\n",
      "Mean Precision:  0.6166666666666667\n",
      "Mean Recall (TPR):  1.0\n",
      "Mean FPR:  1.0\n"
     ]
    }
   ],
   "source": [
    "# This is a plain svc model without any feature selection using all 10 folds\n",
    "auc_list = []\n",
    "acc_list = []\n",
    "prec_list = []\n",
    "tpr_list = []\n",
    "fpr_list = []\n",
    "# Loop through each split, train the model, and print metrics\n",
    "for i in range(len(X_train_list)):\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i]\n",
    "    \n",
    "    # implement model\n",
    "    model = SVC(random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get decision function scores (predict_proba)\n",
    "    y_scores = model.decision_function(X_test) # linear svc does not have a direct predict proba attribute\n",
    "    # Predict labels\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # calculate metrics\n",
    "    # ROC AUC\n",
    "    fprs, tprs, thresholds = roc_curve(y_test, y_scores)\n",
    "    roc_auc = auc(fprs, tprs)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    tpr = recall_score(y_test, y_pred) # redefine as a value not a vector from before\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel() # retrieve to calculate fpr\n",
    "    fpr = fp / (fp + tn)\n",
    "    # append metrics for averages\n",
    "    acc_list.append(accuracy)\n",
    "    auc_list.append(roc_auc)\n",
    "    prec_list.append(precision)\n",
    "    tpr_list.append(tpr)\n",
    "    fpr_list.append(fpr)\n",
    "    \n",
    "    print(f\"Split {i} - AUC: {roc_auc}, Accuracy: {accuracy}, Precision: {precision}, TPR: {tpr}, FPR: {fpr}\")\n",
    "\n",
    "# Report avg metrics\n",
    "print(\"Mean AUC: \", np.mean(auc_list))\n",
    "print(\"Mean Accuracy: \", np.mean(acc_list))\n",
    "print(\"Mean Precision: \", np.mean(prec_list))\n",
    "print(\"Mean Recall (TPR): \", np.mean(tpr_list))\n",
    "print(\"Mean FPR: \", np.mean(fpr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba3516e6-f7f3-4cb5-8e40-3a679463201c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 - AUC: 0.22222222222222224, Accuracy: 0.16666666666666666, Precision: 0.25, TPR: 0.3333333333333333, FPR: 1.0\n",
      "Split 1 - AUC: 0.5555555555555556, Accuracy: 0.5, Precision: 0.5, TPR: 1.0, FPR: 1.0\n",
      "Split 2 - AUC: 0.7777777777777778, Accuracy: 0.6666666666666666, Precision: 0.6, TPR: 1.0, FPR: 0.6666666666666666\n",
      "Split 3 - AUC: 0.125, Accuracy: 0.5, Precision: 0.6, TPR: 0.75, FPR: 1.0\n",
      "Split 4 - AUC: 0.625, Accuracy: 0.5, Precision: 0.6, TPR: 0.75, FPR: 1.0\n",
      "Split 5 - AUC: 0.625, Accuracy: 0.6666666666666666, Precision: 1.0, TPR: 0.5, FPR: 0.0\n",
      "Split 6 - AUC: 1.0, Accuracy: 0.8333333333333334, Precision: 1.0, TPR: 0.75, FPR: 0.0\n",
      "Split 7 - AUC: 1.0, Accuracy: 0.6666666666666666, Precision: 1.0, TPR: 0.5, FPR: 0.0\n",
      "Split 8 - AUC: 0.875, Accuracy: 0.8333333333333334, Precision: 0.8, TPR: 1.0, FPR: 0.5\n",
      "Split 9 - AUC: 0.125, Accuracy: 0.5, Precision: 0.6, TPR: 0.75, FPR: 1.0\n",
      "Mean AUC:  0.5930555555555556\n",
      "Mean Accuracy:  0.5833333333333333\n",
      "Mean Precision:  0.695\n",
      "Mean Recall (TPR):  0.7333333333333333\n",
      "Mean FPR:  0.6166666666666666\n"
     ]
    }
   ],
   "source": [
    "# This is a plain sgd svm model without any feature selection using all 10 folds\n",
    "auc_list = []\n",
    "acc_list = []\n",
    "prec_list = []\n",
    "tpr_list = []\n",
    "fpr_list = []\n",
    "# Loop through each split, train the model, and print metrics\n",
    "for i in range(len(X_train_list)):\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i]\n",
    "    # implement model\n",
    "    model = SGDClassifier(random_state=seed,loss=\"hinge\") # another linear svm\n",
    "    model.fit(X_train, y_train)\n",
    "        \n",
    "    # Get decision function scores (predict_proba)\n",
    "    y_scores = model.decision_function(X_test) # linear svc does not have a direct predict proba attribute\n",
    "    # Predict labels\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # calculate metrics\n",
    "    # ROC AUC\n",
    "    fprs, tprs, thresholds = roc_curve(y_test, y_scores)\n",
    "    roc_auc = auc(fprs, tprs)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    tpr = recall_score(y_test, y_pred) # redefine as a value not a vector from before\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel() # retrieve to calculate fpr\n",
    "    fpr = fp / (fp + tn)\n",
    "    # append metrics for averages\n",
    "    acc_list.append(accuracy)\n",
    "    auc_list.append(roc_auc)\n",
    "    prec_list.append(precision)\n",
    "    tpr_list.append(tpr)\n",
    "    fpr_list.append(fpr)\n",
    "    \n",
    "    print(f\"Split {i} - AUC: {roc_auc}, Accuracy: {accuracy}, Precision: {precision}, TPR: {tpr}, FPR: {fpr}\")\n",
    "\n",
    "# Report avg metrics\n",
    "print(\"Mean AUC: \", np.mean(auc_list))\n",
    "print(\"Mean Accuracy: \", np.mean(acc_list))\n",
    "print(\"Mean Precision: \", np.mean(prec_list))\n",
    "print(\"Mean Recall (TPR): \", np.mean(tpr_list))\n",
    "print(\"Mean FPR: \", np.mean(fpr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a0521ec-64c5-478a-a02b-49c17390fbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 - AUC: 0.11111111111111112, Accuracy: 0.3333333333333333, Precision: 0.3333333333333333, TPR: 0.3333333333333333, FPR: 0.6666666666666666\n",
      "Split 1 - AUC: 0.7777777777777779, Accuracy: 0.8333333333333334, Precision: 0.75, TPR: 1.0, FPR: 0.3333333333333333\n",
      "Split 2 - AUC: 0.6666666666666667, Accuracy: 0.6666666666666666, Precision: 0.6, TPR: 1.0, FPR: 0.6666666666666666\n",
      "Split 3 - AUC: 0.5, Accuracy: 0.5, Precision: 0.6, TPR: 0.75, FPR: 1.0\n",
      "Split 4 - AUC: 0.625, Accuracy: 0.5, Precision: 0.6666666666666666, TPR: 0.5, FPR: 0.5\n",
      "Split 5 - AUC: 0.625, Accuracy: 0.5, Precision: 1.0, TPR: 0.25, FPR: 0.0\n",
      "Split 6 - AUC: 1.0, Accuracy: 1.0, Precision: 1.0, TPR: 1.0, FPR: 0.0\n",
      "Split 7 - AUC: 1.0, Accuracy: 0.8333333333333334, Precision: 1.0, TPR: 0.75, FPR: 0.0\n",
      "Split 8 - AUC: 0.75, Accuracy: 0.8333333333333334, Precision: 0.8, TPR: 1.0, FPR: 0.5\n",
      "Split 9 - AUC: 0.625, Accuracy: 0.5, Precision: 0.6, TPR: 0.75, FPR: 1.0\n",
      "Mean AUC:  0.6680555555555555\n",
      "Mean Accuracy:  0.6499999999999999\n",
      "Mean Precision:  0.7349999999999999\n",
      "Mean Recall (TPR):  0.7333333333333333\n",
      "Mean FPR:  0.4666666666666666\n"
     ]
    }
   ],
   "source": [
    "# This is a plain sgd logistic model without any feature selection using all 10 folds\n",
    "auc_list = []\n",
    "acc_list = []\n",
    "prec_list = []\n",
    "tpr_list = []\n",
    "fpr_list = []\n",
    "# Loop through each split, train the model, and print metrics\n",
    "for i in range(len(X_train_list)):\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i]\n",
    "    # implement model\n",
    "    model = SGDClassifier(random_state=seed,loss=\"log_loss\") # makes logistic with sgd\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get decision function scores (predict_proba)\n",
    "    y_scores = model.decision_function(X_test) # linear svc does not have a direct predict proba attribute\n",
    "    # Predict labels\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # calculate metrics\n",
    "    # ROC AUC\n",
    "    fprs, tprs, thresholds = roc_curve(y_test, y_scores)\n",
    "    roc_auc = auc(fprs, tprs)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    tpr = recall_score(y_test, y_pred) # redefine as a value not a vector from before\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel() # retrieve to calculate fpr\n",
    "    fpr = fp / (fp + tn)\n",
    "    # append metrics for averages\n",
    "    acc_list.append(accuracy)\n",
    "    auc_list.append(roc_auc)\n",
    "    prec_list.append(precision)\n",
    "    tpr_list.append(tpr)\n",
    "    fpr_list.append(fpr)\n",
    "    \n",
    "    print(f\"Split {i} - AUC: {roc_auc}, Accuracy: {accuracy}, Precision: {precision}, TPR: {tpr}, FPR: {fpr}\")\n",
    "\n",
    "# Report avg metrics\n",
    "print(\"Mean AUC: \", np.mean(auc_list))\n",
    "print(\"Mean Accuracy: \", np.mean(acc_list))\n",
    "print(\"Mean Precision: \", np.mean(prec_list))\n",
    "print(\"Mean Recall (TPR): \", np.mean(tpr_list))\n",
    "print(\"Mean FPR: \", np.mean(fpr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be5ab5e-ce2d-49ad-8ff9-c191999e7d75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
